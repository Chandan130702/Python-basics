{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzgk5aTjn5or"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "1.What is a parameter?\n",
        "--> A parameter is a variable that the machine learning model learns from the training data.\n",
        "Parameters are internal to the model and are adjusted during training to minimize the loss function.\n",
        "(e.g., weights in linear regression).\n",
        "\n",
        "2. What is correlation?\n",
        "--> Correlation is a statistical measure that expresses the extent to which two variables change together. It ranges from -1 to +1.\n",
        "\n",
        "What does negative correlation mean?\n",
        "--> Negative correlation means an inverse relationship between two variables: as one increases, the other decreases.\n",
        "\n",
        "3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "--> Machine Learning is a method where systems learn patterns from data and make predictions without explicit programming.\n",
        "The main components in Machine Learning are:\n",
        "\n",
        "> Data: The information used to train and test models\n",
        "> Model: The mathematical representation that learns patterns from data\n",
        "> Features: Input variables or attributes used for making predictions\n",
        "> Target/Label: The output variable that we want to predict\n",
        "> Algorithm: The learning method or technique used to train the model\n",
        "> Loss Function: A metric that measures how far the model's predictions are from actual values\n",
        "> Optimizer: An algorithm that adjusts model parameters to minimize the loss function\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "--> The loss value quantifies the difference between the model's predictions and the actual target values. A lower loss value indicates that the model's predictions are closer to the actual values, suggesting better model performance.\n",
        "Lower loss = better model performance.\n",
        "\n",
        "5.What are continuous and categorical variables?\n",
        "--> Continuous Variables: Numeric values that can take any value within a range (infinite possibilities). Example: Height, Weight, Temperature, Price.\n",
        "Categorical Variables: Variables that represent distinct groups or categories (finite set). Example: Gender (Male/Female), Color (Red/Blue/Green), Yes/No.\n",
        "\n",
        "6.How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "--> Machine Learning models generally require input data to be numerical. They cannot process text or labels directly. Therefore, we must encode these variables into a numeric format before training the model.\n",
        "Common Techniques are:\n",
        "> Label Encoding\n",
        "> One-Hot Encoding\n",
        "> Ordinal Encoding\n",
        "\n",
        "7. What do you mean by training and testing a dataset?\n",
        "--> Training a Dataset: This is the process of feeding data into a model so it can learn patterns and relationships. In Python, this is done using the model.fit() command.\n",
        "> Testing a Dataset: This involves using a separate, unseen subset of data (the Test set) to evaluate how accurately the model makes predictions. This is done using the model.predict() command.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "--> sklearn.preprocessing is a module in the scikit-learn library that provides various utilities and functions for data preprocessing and transformation. It contains tools to prepare raw data for machine learning models.\n",
        "\n",
        "9. What is a Test set?\n",
        "--> A Test set is a specific subset of data that is withheld during the training phase and used exclusively to evaluate the final performance of a Machine Learning model. It represents \"unseen\" data, allowing you to check how well the model generalizes to new, real-world information rather than just memorizing the training data.\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
        "--> In Python, we use the train_test_split function from the sklearn.model_selection library to divide the dataset.\n",
        "\n",
        "Approaching a Machine Learning problem involves a systematic workflow to ensure the model is accurate and reliable:\n",
        "> Define the Goal: Determine what you are trying to predict (the target variable).\n",
        "> Perform EDA: Use Exploratory Data Analysis to find correlations and patterns in the data before modeling.\n",
        "> Data Preprocessing: Clean the data, handle categorical variables, and perform feature scaling using sklearn.preprocessing.\n",
        "> Split Data: Divide your dataset into a Training set and a Test set.\n",
        "> Model Fitting: Use model.fit() to train the algorithm on your training data.\n",
        "> Evaluation: Use model.predict() on the test set to check the accuracy and loss value.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "--> Performing EDA is essential because it allows you to:\n",
        "> Identify Issues: Spot missing values and outliers that would lower model accuracy.\n",
        "> Find Patterns: Discover correlations and relationships between variables.\n",
        "> Select Features: Determine which data is actually relevant to the prediction.\n",
        "> Validate Assumptions: Ensure the data structure fits the chosen algorithm's requirements.\n",
        "\n",
        "12. What is correlation?\n",
        "--> Correlation is a statistical measure that expresses the extent to which two variables change together.\n",
        "\n",
        "> Positive Correlation: Both variables move in the same direction (as one increases, the other increases).\n",
        "> Negative Correlation: Variables move in opposite directions (as one increases, the other decreases).\n",
        "> No Correlation: There is no visible relationship between the changes in the variables.\n",
        "\n",
        "13. What does negative correlation mean?\n",
        "--> Negative correlation (also called inverse correlation) means that two variables move in opposite directions. As one variable increases, the other variable tends to decrease, and vice versa. The relationship is inverse.\n",
        "\n",
        "14. How can you find correlation between variables in Python?\n",
        "--> Pandas .corr() Method: This is the most common way to calculate the correlation matrix for a DataFrame.\n",
        "Seaborn Heatmap: This is used to visually represent the correlation matrix, making it easier to spot strong relationships.\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "--> Causation (or causality) means that one event is the direct result of the occurrence of the other event. It implies a \"cause and effect\" relationship where the first variable actually makes the second variable happen.\n",
        "> Correlation: Simply shows that two variables move together (up or down) at the same time. It does not prove that one causes the other.\n",
        "> Causation: Proves that changing one variable will change the other.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "An optimizer is an algorithm used to update a model’s parameters (weights and bias) in order to minimize the loss function and improve model performance during training.\n",
        "Types of Optimizers:\n",
        "> Gradient Descent (GD): The most basic optimizer that updates parameters in the direction of the steepest decrease of the loss function.\n",
        "Example: Walking down a hill in the dark by feeling the slope under your feet to find the lowest point.\n",
        "> Stochastic Gradient Descent (SGD): A variation that updates parameters using only one random data point at a time, making it faster for large datasets.\n",
        "Example: Checking the slope after every single step rather than waiting to look at the whole mountain.\n",
        "> Adam (Adaptive Moment Estimation): A popular optimizer that calculates individual adaptive learning rates for different parameters.\n",
        "Example: A hiker who adjusts their speed based on how steep or smooth the terrain is at that specific moment.\n",
        "> RMSProp: An optimizer designed to handle non-stationary objectives by using a moving average of squared gradients.\n",
        "Example: Balancing the speed of descent so the model doesn't bounce back and forth too wildly in steep areas.\n",
        "\n",
        "17. What is sklearn.linear_model ?\n",
        "--> sklearn.linear_model is a module in the scikit-learn library that contains various linear models for both regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "--> It triggers the learning process where the algorithm analyzes the provided data to find patterns, relationships, and optimal parameters (like weights and biases). By \"fitting\" the data, the model maps the inputs to the outputs so it can make accurate predictions later.\n",
        "When using scikit-learn, the fit() method typically requires two main arguments:\n",
        "\n",
        "1. X (Features/Independent Variables): The training data or \"input\" variables (usually a 2D array or DataFrame).\n",
        "\n",
        "2. y (Target/Dependent Variable): The labels or \"correct answers\" that the model is trying to learn to predict (usually a 1D array or Series).\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "--> It takes new, unseen input data and uses the trained model to calculate and return the predicted target values. This is the stage where the model is actually \"put to work\" to provide answers for a regression (numerical values) or classification (categories) task.\n",
        "\n",
        "The predict() method typically requires one main argument:\n",
        "\n",
        "X (Features/Independent Variables): This must be the data you want to get predictions for. It should have the same structure (same number of columns and features) as the data used during training.\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "--> Continuous Variables: Continuous variables are numerical variables that can take any value within a given range or interval. They can have infinite possible values, including decimals and fractions. These variables represent measurements or quantities that can be divided into finer and finer increments.\n",
        "Categorical Variables: Categorical variables are variables that represent discrete categories, groups, or labels. They have a limited, fixed number of possible values and represent qualitative characteristics rather than quantities. These variables classify data into distinct groups.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "--> Feature Scaling is a data preprocessing technique used to transform the numerical features of a dataset into a common, standardized range (typically 0 to 1 or -1 to 1). It ensures that no single feature dominates others simply because of its original scale.\n",
        "\n",
        "How it helps in Machine Learning:\n",
        "> Prevents Bias Toward Large Numbers: If one feature ranges from 1 to 10 and another from 1 to 1,000, many algorithms (like KNN or SVM) will mistakenly assume the larger numbers are more important. Scaling puts them on a level playing field.\n",
        "> Speeds up Gradient Descent: For algorithms that use optimization (like Linear Regression or Neural Networks), scaling makes the \"error surface\" more spherical. This allows the Optimizer to find the minimum error much faster.\n",
        "> Improves Accuracy of Distance-Based Models: Algorithms like K-Nearest Neighbors (KNN) or K-Means Clustering rely on measuring the distance between points. Without scaling, the distance calculation would be heavily skewed by features with large magnitudes.\n",
        "\n",
        "Common Techniques:\n",
        "> Normalization (Min-Max Scaling): Rescales the data to a range between 0 and 1.\n",
        "> Standardization (Z-score Scaling): Centers the data around a mean of 0 with a standard deviation of 1.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "--> Feature scaling in Python is performed using the sklearn.preprocessing module, which provides built-in classes to transform numerical features into a common scale.\n",
        "\n",
        "The general process of scaling involves two main steps:\n",
        "> Fitting the scaler: The scaler learns the necessary statistics from the data, such as the minimum and maximum values (for Min–Max scaling) or the mean and standard deviation (for Standardization).\n",
        "> Transforming the data: The learned statistics are then used to rescale the original data into a new standardized or normalized form.\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "--> sklearn.preprocessing is a module in the scikit-learn library that provides a comprehensive collection of utilities and functions for data preprocessing and transformation. It helps convert raw data into a clean, standardized format suitable for machine learning algorithms.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "--> In Python, data is split into training and testing sets using the train_test_split() function from the sklearn.model_selection module.\n",
        "\n",
        "Use train_test_split from sklearn.\n",
        "> Syntax: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "> Logic: Usually 80% for training and 20% for testing.\n",
        "\n",
        "25. Explain data encoding?\n",
        "--> Data Encoding is the process of converting categorical data (text labels like \"Red,\" \"Green,\" \"Blue\") into numerical values.\n",
        "Machine Learning models are based on mathematical equations, so they cannot process text directly; they require numbers to perform calculations.\n",
        "\n",
        "\n",
        "'''"
      ]
    }
  ]
}